{
 "metadata": {
  "name": "",
  "signature": "sha256:d7c602a440a020fc8db87f6f42783b51af833047e274e6ef1785c80db794ffd1"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Implementing a simple perceptron in python - theano version"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pylab as pl\n",
      "import numpy as np\n",
      "import matplotlib.cm as cm\n",
      "import sklearn.cross_validation as skcross\n",
      "%matplotlib inline\n",
      "\n",
      "np.set_printoptions(precision=5, suppress=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Very quick introduction to theano\n",
      "\n",
      "[Theano](http://www.deeplearning.net/software/theano/) is basically a code generator that compiles mathematical expressions into computer code.\n",
      "\n",
      "The basic idea to keep in mind is that when using Theano, you work with *symbolic* variables, like you do when you write mathematical expressions. It is only when you call *theano.function* that code\n",
      "(GPU code if you target the GPU, numpy code if you target the CPU) gets generated to compute your function."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Note that this require a kernel restart to be effective\n",
      "import os\n",
      "os.environ['THEANO_FLAGS'] = 'device=cpu'\n",
      "#os.environ['THEANO_FLAGS'] = 'device=gpu'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import theano.tensor as T\n",
      "import theano\n",
      "import tempfile\n",
      "import pydot\n",
      "from IPython.display import SVG\n",
      "from IPython.display import display\n",
      "\n",
      "print 'device : ', theano.config.device"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def show_computation_graph(fn):\n",
      "    \"\"\"\n",
      "    Wrapper around theano.printing.pydotprint that show the computation graph\n",
      "    for a given theano function\n",
      "    Args:\n",
      "        fn: the function for which to show the computation graph\n",
      "    \"\"\"\n",
      "    display(SVG(theano.printing.pydotprint(fn, return_image=True, format='svg')))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will start with simple theano expressions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### a*x + b"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a = T.scalar('a', dtype='float32')\n",
      "b = T.scalar('b', dtype='float32')\n",
      "x = T.scalar('x', dtype='float32')\n",
      "\n",
      "res = a*x + b\n",
      "\n",
      "# compile the function for our target device (GPU or CPU)\n",
      "res_f = theano.function(inputs=[a, b, x], outputs=res)\n",
      "\n",
      "print res_f(a=2, b=3, x=5)\n",
      "\n",
      "show_computation_graph(res_f)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### sigmoid(x)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = T.scalar('x', dtype='float32')\n",
      "sigmoid_x = T.nnet.sigmoid(x)\n",
      "f_x = theano.function(inputs=[x], outputs=sigmoid_x)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f_x(1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "xvals = np.linspace(-6, 6, num=50, dtype=np.float32)\n",
      "yvals = [f_x(v) for v in xvals]\n",
      "pl.plot(xvals, yvals)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "show_computation_graph(f_x)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Same thing but x is a vector\n",
      "\n",
      "Using a vector for x lets us compute the sigmoid of a vector of values instead of doing it\n",
      "one-by-one"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = T.vector('x', dtype='float32')\n",
      "sigmoid_x = T.nnet.sigmoid(x)\n",
      "f_x = theano.function(inputs=[x], outputs=sigmoid_x)\n",
      "# without a for\n",
      "yvals = f_x(xvals)\n",
      "pl.plot(xvals, yvals)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "show_computation_graph(f_x)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Perceptron implementation\n",
      "\n",
      "We build a perceptron with a sigmoid activation function. That is\n",
      "\n",
      "$$o_i = sigmoid(W*x_i + b)$$\n",
      "\n",
      "Where $sigmoid(x) = \\frac{1}{1 + e^{-x}}$\n",
      "\n",
      "The error is simply ($y_i$ being the expected output value)\n",
      "\n",
      "$$e_i = (y_i - o_i)^2$$\n",
      "\n",
      "We train the network with minibatch SGD. That is, at each epoch, we compute the average gradient over a bunch of examples as opposed to a single one.\n",
      "\n",
      "$$W^{t+1} = W^t - \\eta \\frac{\\partial e_i}{\\partial W}$$\n",
      "$$b^{t+1} = b^t - \\eta \\frac{\\partial e_i}{\\partial b}$$\n",
      "\n",
      "Where $\\eta$ is the learning rate."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Error function derivative\n",
      "\n",
      "Note that if we add an entry set to 1 at the end of the x vector, we can transform our $Wx + b$ into $W\\hat{x} + b$ where $\\hat{x} = \\left[\\begin{matrix}x \\\\ 1 \\end{matrix}\\right]$\n",
      "\n",
      "$$l_i = W\\hat{x}$$\n",
      "\n",
      "$$o_i = sigmoid(l_i)$$\n",
      "\n",
      "$$e_i = (y_i - o_i)^2$$\n",
      "\n",
      "We want to compute the derivative of $e_i$ w.r.t $W$. From the [chain rule](http://en.wikipedia.org/wiki/Chain_rule), we have\n",
      "\n",
      "\n",
      "$$\\frac{\\partial e_i}{\\partial W} = \\frac{\\partial e_i}{\\partial o_i} \\frac{\\partial o_i}{\\partial l_i} \\frac{\\partial l_i}{\\partial W}$$\n",
      "\n",
      "With\n",
      "$$\\frac{\\partial e_i}{\\partial o_i} = -2(y_i - o_i)$$\n",
      "$$\\frac{\\partial o_i}{\\partial l_i} = sigmoid(l_i)(1 - sigmoid(l_i))$$\n",
      "$$\\frac{\\partial l_i}{\\partial W} = \\hat{x}$$\n",
      "\n",
      "Which gives us\n",
      "\n",
      "$$\\frac{\\partial e_i}{\\partial W} = -2(y_i - o_i) sigmoid(l_i)(1 - sigmoid(l_i)) \\hat{x}$$\n",
      "\n",
      "Which we use to update our weights at each step of gradient descent :\n",
      "\n",
      "$$W^{t+1} = W^t - \\frac{\\partial e_i}{\\partial W}$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# number of features\n",
      "X_dim = 2\n",
      "\n",
      "# weights\n",
      "W_init = 2 * (np.random.random(X_dim) - 0.5)\n",
      "W = theano.shared(\n",
      "    value=W_init.reshape(1, -1),\n",
      "    name='W',\n",
      "    borrow=True\n",
      ")\n",
      "\n",
      "# bias\n",
      "b_init = 2 * (np.random.random() - 0.5)\n",
      "b = theano.shared(\n",
      "    value = b_init,\n",
      "    name='b',\n",
      ")\n",
      "\n",
      "# data matrix\n",
      "X = T.matrix('X')\n",
      "# labels vector\n",
      "y = T.vector('y')\n",
      "\n",
      "y_pred = T.nnet.sigmoid(T.dot(W, X.T) + b)\n",
      "\n",
      "error = T.mean(T.sqr(y - y_pred))\n",
      "\n",
      "# compilation\n",
      "y_pred_fn = theano.function(\n",
      "    inputs=[X],\n",
      "    outputs=y_pred\n",
      ")\n",
      "\n",
      "error_fn = theano.function(\n",
      "    inputs=[X, y],\n",
      "    outputs=error\n",
      ")\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# You can explore the computation graph as follow :\n",
      "if False:\n",
      "    show_computation_graph(error)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# A forward pass through our perceptron\n",
      "error_fn(X=[[0, 1], [0, 2], [3, 4]], y=[0, 1, 2])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# For backprop, we need the gradient with regard to our weights and bias\n",
      "g_W = T.grad(cost=error, wrt=W)\n",
      "g_b = T.grad(cost=error, wrt=b)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# We can compute the gradient wrt the weights as follow\n",
      "g_W_fn = theano.function(\n",
      "    inputs=[X, y],\n",
      "    outputs=g_W\n",
      ")\n",
      "\n",
      "# Gradient for X=[0, 1] and y=0\n",
      "g_W_fn(X=[[0, 1]], y=[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# same for bias\n",
      "g_b_fn = theano.function(\n",
      "    inputs=[X, y],\n",
      "    outputs=g_b\n",
      ")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can check that theano's computed gradient match the derivation we made by hand.\n",
      "\n",
      "Note that this seems to run into numerical accuracy issues."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def sigmoid(x):\n",
      "    return 1 / (1 + np.exp(-x))\n",
      "\n",
      "def my_grad(X, y, W, b):\n",
      "    o = sigmoid(W.dot(X) + b)\n",
      "    # The last X is the derivative of o wrt X, which is X\n",
      "    gW = -2*(y - o)*sigmoid(o)*(1 - sigmoid(o))*X\n",
      "    # The last 1 is the derivative of o wrt b, which is 1\n",
      "    gb = -2*(y - o)*sigmoid(o)*(1 - sigmoid(o))*1\n",
      "    \n",
      "    return gW, gb\n",
      "\n",
      "X1 = np.array([0, 1])\n",
      "y1 = np.array([0])\n",
      "print my_grad(X1, y1, W.get_value(), b.get_value())\n",
      "print g_W_fn([X1], y1), g_b_fn([X1], y1)\n",
      "\n",
      "print '--'\n",
      "X1 = np.array([-0.2, 0.7])\n",
      "y1 = np.array([0.5])\n",
      "print my_grad(X1, y1, W.get_value(), b.get_value())\n",
      "print g_W_fn([X1], y1), g_b_fn([X1], y1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We could use g_W_fn to loop over our samples and update our weights. But theano has a nice *updates* parameters that you can give to the *theano.function*. It specifies which variables should be updated and how."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "learning_rate = 1\n",
      "updates = [\n",
      "    (W, W - learning_rate * g_W),\n",
      "    (b, b - learning_rate * g_b)\n",
      "]\n",
      "\n",
      "train_model = theano.function(\n",
      "    inputs=[X, y],\n",
      "    outputs=error,\n",
      "    updates=updates\n",
      ")\n",
      "\n",
      "predict = theano.function(\n",
      "    inputs=[X],\n",
      "    outputs=y_pred\n",
      ")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "W.set_value([[0.5, 0.5]])\n",
      "b.set_value(0)\n",
      "\n",
      "X1 = [[0, 1]]\n",
      "y1 = [0]\n",
      "print \"prediction        \", predict(X1)\n",
      "for i in xrange(5):\n",
      "    print '---- iteration', i\n",
      "    print \"W, b before       \", W.get_value(), b.get_value()\n",
      "    print \"gradient wrt W, b \", g_W_fn(X=X1, y=y1), g_b_fn(X=X1, y=y1)\n",
      "    train_model(X=X1, y=y1)\n",
      "    print \"W, b after updates\", W.get_value(), b.get_value()\n",
      "    print \"prediction        \", predict(X1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Important note\n",
      "\n",
      "Note that this is a naive implementation and it will suffer from one major problem if ran onto the GPU : the data (X) will be uploaded to the GPU on every call to our functions. This will most likely using the GPU will be slower than a CPU for this particular implementation.\n",
      "\n",
      "Of course, Theano allows you to avoid this and upload your data once to the GPU. The [logistic regression theano tutorial](http://deeplearning.net/tutorial/logreg.html) is a good reference for how to do minibatch training with the data in GPU memory. The key is to upload X and y once and then simply provide the index of the sample as an argument to our various functions."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Let's put everything in a class"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is a perceptron class that contains the same code as above, for ease-of-reuse"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class Perceptron(object):\n",
      "    def __init__(self, X_dim, learning_rate=0.2):\n",
      "        # theano variable definitions\n",
      "        W_init = (2 * np.random.random(X_dim)) * 0.25\n",
      "        self.W = theano.shared(\n",
      "            value=W_init.reshape(1, -1),\n",
      "            name='W',\n",
      "            borrow=True\n",
      "        )\n",
      "        b_init = (2 * np.random.random()) * 0.25\n",
      "        self.b = theano.shared(\n",
      "            value = b_init,\n",
      "            name='b',\n",
      "        )\n",
      "\n",
      "        self.X = T.matrix('X')\n",
      "        self.y = T.vector('y')\n",
      "\n",
      "        self.y_pred = T.nnet.sigmoid(T.dot(self.W, self.X.T) + self.b)\n",
      "\n",
      "        self.error = T.mean(T.sqr(self.y - self.y_pred))\n",
      "        \n",
      "        self.g_W = T.grad(cost=self.error, wrt=self.W)\n",
      "        self.g_b = T.grad(cost=self.error, wrt=self.b)\n",
      "        \n",
      "        # theano functions\n",
      "        self.predict_fn = theano.function(\n",
      "            inputs=[self.X],\n",
      "            outputs=self.y_pred\n",
      "        )\n",
      "        \n",
      "        self.error_fn = theano.function(\n",
      "            inputs=[self.X, self.y],\n",
      "            outputs=self.error\n",
      "        )\n",
      "        \n",
      "        updates = [\n",
      "            (self.W, self.W - learning_rate * self.g_W),\n",
      "            (self.b, self.b - learning_rate * self.g_b)\n",
      "        ]\n",
      "\n",
      "        self.train_model_fn = theano.function(\n",
      "            inputs=[self.X, self.y],\n",
      "            outputs=self.error,\n",
      "            updates=updates\n",
      "        )\n",
      "        \n",
      "        # non-theano stuff\n",
      "        self.train_errors = []\n",
      "        self.validation_errors = []\n",
      "        \n",
      "    def train_gd(self, X_train, y_train, X_validation, y_validation, epochs=10):\n",
      "        \"\"\"Simple gradient descent training\"\"\"\n",
      "        assert len(X_train.shape) == 2, \"X must be 2D\"\n",
      "        \n",
      "        for epoch in range(epochs):\n",
      "            # This does forward propagation, gradient computation and weights\n",
      "            # update all in one\n",
      "            self.train_model_fn(X_train, y_train)\n",
      "            \n",
      "            # evaluate train/validation errors\n",
      "            self.train_errors.append(\n",
      "                self.error_fn(X_train, y_train)\n",
      "            )\n",
      "            self.validation_errors.append(\n",
      "                self.error_fn(X_validation, y_validation)\n",
      "            )\n",
      "        \n",
      "        return self\n",
      "                    \n",
      "    def predict(self, X):\n",
      "        \"\"\"Binary (0, 1) prediction\"\"\"\n",
      "        return self.predict_fn(X).astype(np.int)\n",
      "\n",
      "    def decision_function(self, X):\n",
      "        \"\"\"Float [0,1] prediction\"\"\"\n",
      "        return self.predict_fn(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Generate a toy dataset and test"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def dataset_fixed_cov():\n",
      "    \"\"\"Generate 2 Gaussians samples with the same covariance matrix\"\"\"\n",
      "    n, dim = 300, 2\n",
      "    np.random.seed(42)\n",
      "    C = np.array([[0., -0.23], [0.83, .23]])\n",
      "    X = np.r_[np.dot(np.random.randn(n, dim), C),\n",
      "              np.dot(np.random.randn(n, dim), C) + np.array([1, 1])]\n",
      "    y = np.hstack((np.zeros(n), np.ones(n))).astype(np.int)\n",
      "    return X, y\n",
      "\n",
      "X, y = dataset_fixed_cov()\n",
      "\n",
      "print X"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pl.scatter(X[y==0, 0], X[y==0, 1], c='r')\n",
      "pl.scatter(X[y==1, 0], X[y==1, 1], c='b')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train, X_validation, y_train, y_validation = skcross.train_test_split(X, y, test_size=0.33, random_state=20)\n",
      "perceptron = Perceptron(X.shape[1], learning_rate=0.2)\n",
      "perceptron.train_gd(X_train, y_train, X_validation, y_validation, epochs=800)\n",
      "#perceptron.train_minisgd(X_train, y_train, X_validation, y_validation, epochs=500, minibatch_size=10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pl.plot(perceptron.train_errors, label='train')\n",
      "pl.plot(perceptron.validation_errors, label='validation')\n",
      "pl.legend()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
      "import matplotlib.gridspec as gridspec\n",
      "\n",
      "def show_decision_boundary(clf, X, y, subplot_spec=None):\n",
      "    \"\"\"\n",
      "    Utility function to plot the decision function of a classifier\n",
      "    \"\"\"\n",
      "    assert X.shape[1] == 2\n",
      "    wratio = (15, 1)\n",
      "    if subplot_spec is None:\n",
      "        gs = gridspec.GridSpec(1,2, width_ratios=wratio)\n",
      "    else:\n",
      "        gs = gridspec.GridSpecFromSubplotSpec(1, 2, subplot_spec=subplot_spec, width_ratios=wratio)\n",
      "        \n",
      "    ax = pl.subplot(gs[0])\n",
      "    ax.set_title('Dataset and decision function')\n",
      "    \n",
      "    x_min, x_max = X[:,0].min() - 1, X[:,0].max() + 1\n",
      "    y_min, y_max = X[:,1].min() - 1, X[:,1].max() + 1\n",
      "    h = 0.2 # step size in the meshgrid\n",
      "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
      "                         np.arange(y_min, y_max, h))\n",
      "\n",
      "    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
      "    Z = Z.reshape(xx.shape)\n",
      "    ctr = ax.contourf(xx, yy, Z, cmap=cm.gray, vmin=0, vmax=1)\n",
      "    \n",
      "    unique_labels = np.unique(y)\n",
      "    colors = cm.Paired(np.linspace(0, 1, num=len(unique_labels)))\n",
      "    for i, yi in enumerate(unique_labels):\n",
      "        color = colors[i]\n",
      "        ax.scatter(X[y == yi, 0], X[y == yi, 1], c=color, linewidth=0, label='%d' % yi)\n",
      "    ax.legend()\n",
      "    ax.set_xlim((x_min, x_max))\n",
      "    ax.set_ylim((y_min, y_max))\n",
      "\n",
      "    pl.colorbar(ctr, cax=pl.subplot(gs[1]))\n",
      "\n",
      "show_decision_boundary(perceptron, X, y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Making a video of the training\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.animation as manimation\n",
      "from tempfile import NamedTemporaryFile\n",
      "from matplotlib.legend_handler import HandlerLine2D\n",
      "\n",
      "#AVConvWriter = manimation.writers['avconv']\n",
      "AVConvWriter = manimation.writers['ffmpeg']\n",
      "metadata = dict(title='Perceptron training', artist='',comment='')\n",
      "writer = AVConvWriter(fps=4, metadata=metadata, codec=\"libx264\", extra_args=['-vcodec', 'libx264'])\n",
      "perceptron = Perceptron(X.shape[1])\n",
      "\n",
      "fig = pl.figure(figsize=(10, 5))\n",
      "gs = gridspec.GridSpec(1, 2, wspace=0.4)\n",
      "\n",
      "nepochs = 800\n",
      "\n",
      "err_ymax = None\n",
      "\n",
      "with NamedTemporaryFile(suffix='.mp4') as f:\n",
      "    with writer.saving(fig, f.name, 100):\n",
      "        for epoch in xrange(nepochs):\n",
      "            perceptron.train_gd(X_train, y_train, X_validation, y_validation, epochs=1)\n",
      "            #perceptron.train_minisgd(X_train, y_train, X_validation, y_validation, epochs=1, minibatch_size=10)\n",
      "\n",
      "            if err_ymax is None:\n",
      "                err_ymax = max(np.max(perceptron.train_errors),\n",
      "                               np.max(perceptron.validation_errors)) * 1.1\n",
      "\n",
      "            if epoch % 10 == 0: # Capturing all 800 epochs takes forever...\n",
      "                show_decision_boundary(perceptron, X, y, gs[0])\n",
      "\n",
      "                ax = pl.subplot(gs[1])\n",
      "                pl.title('Error')\n",
      "                line_train, = ax.plot(np.array(perceptron.train_errors), c='b', label='train')\n",
      "                line_valid, = ax.plot(np.array(perceptron.validation_errors), c='g', label='validation')\n",
      "                ax.set_xlim(0, nepochs)\n",
      "                ax.set_ylim(0, err_ymax)\n",
      "                ax.set_xlabel('epochs')\n",
      "                ax.set_ylabel('error')\n",
      "                ax.legend((line_train, line_valid), ('train', 'validation'))\n",
      "\n",
      "                writer.grab_frame()\n",
      "            \n",
      "    video = open(f.name, \"rb\").read()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.display import HTML\n",
      "\n",
      "VIDEO_TAG = \"\"\"<video style=\"width:80%\" controls>\n",
      " <source src=\"data:video/x-m4v;base64,{0}\" type=\"video/mp4\">\n",
      " Your browser does not support the video tag.\n",
      "</video>\"\"\"\n",
      "\n",
      "HTML(data=VIDEO_TAG.format(video.encode(\"base64\")))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Interesting links\n",
      "\n",
      "Tutorials :\n",
      "\n",
      "[Theano tutorials](http://deeplearning.net/software/theano/tutorial/)\n",
      "\n",
      "[Deep learning tutorials using Theano](http://deeplearning.net/tutorial/contents.html)\n",
      "\n",
      "Deep learning libraries using theano :\n",
      "\n",
      "[Lasagne](https://github.com/Lasagne/Lasagne)\n",
      "\n",
      "[Blocks](https://github.com/bartvm/blocks)\n",
      "\n",
      "[pylearn2](http://deeplearning.net/software/pylearn2/)\n",
      "\n",
      "General deep learning :\n",
      "\n",
      "[Reading recommendations](http://deeplearning.net/reading-list/)\n",
      "\n",
      "[Software](http://deeplearning.net/software_links/)\n",
      "\n",
      "[Stanford course about CNN](http://cs231n.stanford.edu/)\n",
      "\n",
      "Cool demos :\n",
      "\n",
      "[CNN in your browser](http://cs.stanford.edu/people/karpathy/convnetjs/)\n",
      "\n",
      "Misc :\n",
      "\n",
      "[How to access theano generated code](https://groups.google.com/forum/#!topic/theano-users/o-BLhLvnL5s)\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Additional stuff\n",
      "\n",
      "To add minibatch SGD training to the perceptron, use the following code. Note that since we do more weight updates per epoch (since we update for each minibatch), this requires less epoch to converge.\n",
      "\n",
      "    def train_minisgd(self, X_train, y_train, X_validation, y_validation, epochs=10, minibatch_size=10):\n",
      "        \"\"\"Training with minibatch SGD\"\"\"\n",
      "        assert len(X_train.shape) == 2, \"X must be 2D\"\n",
      "        \n",
      "        for epoch in range(epochs):\n",
      "            minibatch_indices = np.arange(X_train.shape[0])\n",
      "            np.random.shuffle(minibatch_indices)\n",
      "            # for each minibatch, compute gradient of weights\n",
      "            for start in xrange(0, len(minibatch_indices), minibatch_size):\n",
      "                end = start + minibatch_size\n",
      "                indices = minibatch_indices[start:end]\n",
      "                Xb = X_train[indices]\n",
      "                yb = y_train[indices]\n",
      "                \n",
      "                self.train_model_fn(Xb, yb)\n",
      "            \n",
      "            # evaluate train/validation errors\n",
      "            self.train_errors.append(\n",
      "                self.error_fn(X_train, y_train)\n",
      "            )\n",
      "            self.validation_errors.append(\n",
      "                self.error_fn(X_validation, y_validation)\n",
      "            )\n",
      "        \n",
      "        return self"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}